{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework exercise 3\n",
    "## Deadline: upload to Moodle by 7 June 18:00 h\n",
    "\n",
    "__Please submit your homework either as a Jupyter Notebook or using .py files.__\n",
    "\n",
    "If you use .py files, please also include a PDF containing the output of your code and your explanations. Either way, the code needs to be in a form that can be easily run on another computer.\n",
    "\n",
    "__Name 1:__\n",
    "\n",
    "__Name 2:__\n",
    "\n",
    "__Name 3:__\n",
    "\n",
    "\n",
    "The name of the file that you upload should be named *Homework1_YourLastName_YourStudentID*.\n",
    "\n",
    "Reminder: you are required to attend class on 26 May to earn points for this homework exercise unless you have a valid reason for your absence.\n",
    "\n",
    "You are encouraged to work on this exercise in teams of up to three students. If any part of the questions is unclear, please ask on the Moodle forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIs, NLP, ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reddit__\n",
    "\n",
    "\n",
    "The Reddit offers an API providing free access to up to 1000 submissions (i.e. posts), comments on those submissions, and some meta data. `PRAW` is a module that facilitates usage of the Reddit API, and it is recommended that you use it. It is documented at https://praw.readthedocs.io/en/latest/index.html and the Quick Start page should give you most of the information you need.\n",
    "\n",
    "(If you would like to use more Reddit data, you may want to consider `PSAW`. You won't need it for this homework exercise, though it might be of interest if you'd like to use Reddit in your course project.)\n",
    "\n",
    "Please note that using the Reddit API requires users to register. Feel free to use any temporary email address if you have any concerns with sharing your usual address with Reddit.\n",
    "\n",
    "Please collect data for the top 1000 submissions and all the comments on those submissions for the subreddits 'wallstreetbets' and 'StockMarket'.\n",
    "\n",
    "The goal will be to classify submissions, i.e., to predict which subreddit each submission belongs to. \n",
    "\n",
    "Write a class RedditClassifier that contains two object attributes: the categorical variable to be used in the classification (here: the subreddit) and a list (for now, since you have only encountered a binary classifier, of length 2) of values for that category (i.e. a list of the names of the two subreddits). The class should be derived from the Perceptron class from Chapter 7. It should contain a method that combines the preprocessing and the estimation.\n",
    "\n",
    "The preprocessing should generate features that might help predict the category. Please don't use features such as the frequency of each word as we will take a look at appropriate ways of working with such information only in the next class. Possible features could be: length of title, number of comments, average sentence length, word length, number of sentences in the comments, shares of nouns, verbs, etc., number of stop words used compared to other words, etc.. Please add any other features that you think might be useful for this task but don't include features that are likely to perfectly predict the category (e.g. the author of a submission).\n",
    "\n",
    "Please use this class to classify the submissions. Plot the percentage of submissions that are incorrectly classified in each epoch for different learning rates. Don't expect to achieve near-perfect predictions as the features you employ may not be able to easily distinguish between categories the way they did in the example in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from praw.models import MoreComments\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"cuDmFCTelGCG4w\",\n",
    "    client_secret=\"JJKHv_s7LlxkTKJE90xkPMiZIEqFRQ\",\n",
    "    user_agent=\"Homework3\"\n",
    ")\n",
    "\n",
    "wallstreetbets=reddit.subreddit('wallstreetbets')\n",
    "stockmarket=reddit.subreddit('StockMarket')\n",
    "\n",
    "\n",
    "hot_stockmarket=stockmarket.hot(limit=1000)\n",
    "\n",
    "hot_wsb=wallstreetbets.hot(limit=1000)\n",
    "\n",
    "wsb_list = [submission for submission in hot_wsb]\n",
    "stockmarket_list = [submission for submission in hot_stockmarket]\n",
    "y = [\"stockmarket\"]*len(stockmarket_list) +[\"wsb\"]*len(wsb_list)\n",
    "X = stockmarket_list + wsb_list\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Submission(id='nr21l5'), Submission(id='npwihj'), Submission(id='nqsnfj'), Submission(id='nqsmyo'), Submission(id='nqbqpl'), Submission(id='nquht9'), Submission(id='nq7pr2'), Submission(id='nn0gmj'), Submission(id='nk5x7d'), Submission(id='nq41o1')]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for x in X[:10]:\n",
    "    x.comments.replace_more(limit=0)\n",
    "    comments.append([top_level_comment for top_level_comment in x.comments[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[Comment(id='h0e5hp5'), Comment(id='h0dgt13'), Comment(id='h0dtr7g'), Comment(id='h0dbom8'), Comment(id='h0dtmf9'), Comment(id='h0dwz76'), Comment(id='h0dev6e'), Comment(id='h0drjt8'), Comment(id='h0dtpcl'), Comment(id='h0e4c3b')], [Comment(id='h0cf1mh'), Comment(id='h0c4mhc'), Comment(id='h0bx3cs'), Comment(id='h0c4oj1'), Comment(id='h0c89kh'), Comment(id='h0c7sss'), Comment(id='h0c3bcm'), Comment(id='h0c1qf0'), Comment(id='h0bxolz'), Comment(id='h0bxcdu')], [Comment(id='h0cp9qd'), Comment(id='h0cv1l7'), Comment(id='h0covrf'), Comment(id='h0e1cx5'), Comment(id='h0cxzap'), Comment(id='h0cut78'), Comment(id='h0drwld'), Comment(id='h0e0jwa'), Comment(id='h0cnu8u'), Comment(id='h0coqp4')], [Comment(id='h0dtubt'), Comment(id='h0dbzw1'), Comment(id='h0de111'), Comment(id='h0e1swz'), Comment(id='h0e33bm'), Comment(id='h0ddfs8'), Comment(id='h0dqvlo'), Comment(id='h0ejdy2'), Comment(id='h0dbwz7'), Comment(id='h0dg91k')], [Comment(id='h0bhrho'), Comment(id='h0bjheo'), Comment(id='h0b1dcx'), Comment(id='h0bq091'), Comment(id='h0bjakm'), Comment(id='h0beshh'), Comment(id='h0br993'), Comment(id='h0bp4m0'), Comment(id='h0bv5d6'), Comment(id='h0b5au3')], [Comment(id='h0dclib'), Comment(id='h0d3uod'), Comment(id='h0d1qj0'), Comment(id='h0d6ndq'), Comment(id='h0d2q4e'), Comment(id='h0d1yl0'), Comment(id='h0e7yvb'), Comment(id='h0d2r64'), Comment(id='h0dbmwa'), Comment(id='h0dg5ls')], [Comment(id='h0e7ena'), Comment(id='h0eduxc'), Comment(id='h0elk0u'), Comment(id='h0ems0d')], [Comment(id='h0cur8n')], [Comment(id='h0ej48m'), Comment(id='h0ej8gj'), Comment(id='h0ejbhg'), Comment(id='h0ejwgj'), Comment(id='h0el6ba')], [Comment(id='h09rhiw'), Comment(id='h09yiac'), Comment(id='h09od04'), Comment(id='h09wiio'), Comment(id='h09oo73'), Comment(id='h0af3a7'), Comment(id='h0b8ctr'), Comment(id='h09tsyy'), Comment(id='h0as7ku'), Comment(id='h09rwlo')]]\n"
     ]
    }
   ],
   "source": [
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "    Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "    Passes over the training dataset.\n",
    "    random_state : int\n",
    "    Random number generator seed for random weight\n",
    "    initialization.\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "    Weights after fitting.\n",
    "    errors_ : list\n",
    "    Number of misclassifications (updates) in each epoch.\n",
    "    \"\"\"\n",
    "    def preprocessing(self, X, y):\n",
    "        \n",
    "\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "        Training vectors, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "        Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "        for xi, target in zip(X, y):\n",
    "            update = self.eta * (target - self.predict(xi))\n",
    "            self.w_[1:] += update * xi\n",
    "            self.w_[0] += update\n",
    "            errors += int(update != 0.0)\n",
    "        self.errors_.append(errors)\n",
    "        return self\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd0d3adbaf49ae7f507e83f0ccfbb2546a5cc19668fb515a7486c42bc7952ccb80e",
   "display_name": "Python 3.7.4  ('Finance': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "d3adbaf49ae7f507e83f0ccfbb2546a5cc19668fb515a7486c42bc7952ccb80e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}