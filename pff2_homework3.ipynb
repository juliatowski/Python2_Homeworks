{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework exercise 3\n",
    "## Deadline: upload to Moodle by 7 June 18:00 h\n",
    "\n",
    "__Please submit your homework either as a Jupyter Notebook or using .py files.__\n",
    "\n",
    "If you use .py files, please also include a PDF containing the output of your code and your explanations. Either way, the code needs to be in a form that can be easily run on another computer.\n",
    "\n",
    "__Name 1:__\n",
    "\n",
    "__Name 2:__\n",
    "\n",
    "__Name 3:__\n",
    "\n",
    "\n",
    "The name of the file that you upload should be named *Homework1_YourLastName_YourStudentID*.\n",
    "\n",
    "Reminder: you are required to attend class on 26 May to earn points for this homework exercise unless you have a valid reason for your absence.\n",
    "\n",
    "You are encouraged to work on this exercise in teams of up to three students. If any part of the questions is unclear, please ask on the Moodle forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APIs, NLP, ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reddit__\n",
    "\n",
    "\n",
    "The Reddit offers an API providing free access to up to 1000 submissions (i.e. posts), comments on those submissions, and some meta data. `PRAW` is a module that facilitates usage of the Reddit API, and it is recommended that you use it. It is documented at https://praw.readthedocs.io/en/latest/index.html and the Quick Start page should give you most of the information you need.\n",
    "\n",
    "(If you would like to use more Reddit data, you may want to consider `PSAW`. You won't need it for this homework exercise, though it might be of interest if you'd like to use Reddit in your course project.)\n",
    "\n",
    "Please note that using the Reddit API requires users to register. Feel free to use any temporary email address if you have any concerns with sharing your usual address with Reddit.\n",
    "\n",
    "Please collect data for the top 1000 submissions and all the comments on those submissions for the subreddits 'wallstreetbets' and 'StockMarket'.\n",
    "\n",
    "The goal will be to classify submissions, i.e., to predict which subreddit each submission belongs to. \n",
    "\n",
    "Write a class RedditClassifier that contains two object attributes: the categorical variable to be used in the classification (here: the subreddit) and a list (for now, since you have only encountered a binary classifier, of length 2) of values for that category (i.e. a list of the names of the two subreddits). The class should be derived from the Perceptron class from Chapter 7. It should contain a method that combines the preprocessing and the estimation.\n",
    "\n",
    "The preprocessing should generate features that might help predict the category. Please don't use features such as the frequency of each word as we will take a look at appropriate ways of working with such information only in the next class. Possible features could be: length of title, number of comments, average sentence length, word length, number of sentences in the comments, shares of nouns, verbs, etc., number of stop words used compared to other words, etc.. Please add any other features that you think might be useful for this task but don't include features that are likely to perfectly predict the category (e.g. the author of a submission).\n",
    "\n",
    "Please use this class to classify the submissions. Plot the percentage of submissions that are incorrectly classified in each epoch for different learning rates. Don't expect to achieve near-perfect predictions as the features you employ may not be able to easily distinguish between categories the way they did in the example in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test name 2\n"
     ]
    }
   ],
   "source": [
    "# from privat import username\n",
    "%store -r username\n",
    "print(username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37464biteconomicsvenv07604bd689d349069ff90a37271a5757",
   "display_name": "Python 3.7.4  ('economics': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "ca1ebe3d14de383a5a539e7a1857695c759fc0ad0262b23c222edf48127bb2f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}